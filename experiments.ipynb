{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#%pip uninstall --yes torchaudio\n",
    "#%pip install -U torch torchvision torchdata --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install -U lightning\n",
    "#%pip install wandb\n",
    "#%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v3 as iio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "torchvision.disable_beta_transforms_warning();\n",
    "\n",
    "import torchvision.transforms.v2 as t\n",
    "\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, WandbLogger\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv();\n",
    "\n",
    "from hyperparameters import Hyperparameters\n",
    "from datamodules import ImagenetteDataModule\n",
    "from models import AlexNet\n",
    "\n",
    "#import logging\n",
    "#logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "#logging.getLogger(\"wandb\").setLevel(logging.WARNING)\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = (Path.cwd() / \"experiments.ipynb\").as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENETTE = Path.home() / \"datasets\" / \"imagenette\"\n",
    "\n",
    "CHECKPOINTS_DIR = Path.cwd() / \"checkpoints\"\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "LOGS_DIR = Path.cwd() / \"logs\"\n",
    "LOGS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(LightningModule):\n",
    "    def __init__(self, model, params: Hyperparameters):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        #TODO : Add dicts for Metrics, Optimizers, Criterions\n",
    "        self.task = params.task\n",
    "        self.num_classes = params.num_classes\n",
    "        self.criterion = params.criterion\n",
    "        self.optimizer = params.optimizer\n",
    "        self.learning_rate = params.learning_rate\n",
    "        self.momentum = params.momentum\n",
    "        self.weight_decay = params.weight_decay\n",
    "\n",
    "        self._set_metrics()\n",
    "        self.save_hyperparameters(\n",
    "            {i:params.get_litmodule_hparams()[i] \n",
    "             for i in params.get_litmodule_hparams().keys() if i!='criterion'},\n",
    "            ignore = [\"model\", \"params\"]\n",
    "        ) \n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, _ = batch\n",
    "        return self.model(x)\n",
    "\n",
    "    def _forward_pass(self, batch, metrics : Callable | None = None) -> tuple:\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        if metrics:\n",
    "            metrics.update(y_pred, y) \n",
    "        return self.criterion(y_pred, y), y_pred #type: ignore\n",
    "\n",
    "    def _set_metrics(self):\n",
    "        metrics = {\"accuracy\": torchmetrics.Accuracy(\n",
    "                                task=\"multiclass\",\n",
    "                                num_classes=self.num_classes, \n",
    "                                average=\"macro\"),\n",
    "\n",
    "                   \"f1\" : torchmetrics.F1Score(\n",
    "                                task=\"multiclass\",\n",
    "                                num_classes=self.num_classes,\n",
    "                                average=\"macro\")\n",
    "                  }\n",
    "\n",
    "        self.val_losses = list()\n",
    "        self.val_metrics = torchmetrics.MetricCollection(\n",
    "            metrics=metrics.copy(), prefix = \"val_\"\n",
    "        )\n",
    "        self.val_confm = torchmetrics.ConfusionMatrix(\n",
    "            task = \"multiclass\", num_classes = self.num_classes,\n",
    "        )\n",
    "\n",
    "        self.test_metrics = torchmetrics.MetricCollection(\n",
    "            metrics=metrics.copy(), prefix = \"test_\"\n",
    "        )\n",
    "        self.test_confm = torchmetrics.ConfusionMatrix(\n",
    "            task = \"multiclass\", num_classes = self.num_classes,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._forward_pass(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, y = batch\n",
    "        loss, y_pred = self._forward_pass(batch)\n",
    "        self.val_losses.append(loss)\n",
    "        self.val_metrics.update(y_pred, y)\n",
    "        self.val_confm.update(y_pred, y)\n",
    "        #self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        #self.log_dict(self.val_metrics, on_epoch=True, on_step=False) \n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        val_losses_t = torch.tensor(self.val_losses)\n",
    "        self.log(\"val_loss\", val_losses_t.mean())\n",
    "        self.val_losses.clear()\n",
    "\n",
    "        self.log_dict(self.val_metrics.compute())\n",
    "        self.val_metrics.reset()\n",
    "\n",
    "        self.val_confm.compute()\n",
    "        fig, _ = self.val_confm.plot();\n",
    "        self.val_confm.reset()\n",
    "        for logger in self.loggers:\n",
    "            if isinstance(logger, WandbLogger):\n",
    "                wandb.log({\"val_confusion\": fig})\n",
    "        plt.clf();\n",
    "        #del fig\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, y = batch\n",
    "        _, y_pred = self._forward_pass(batch)\n",
    "        self.test_metrics.update(y_pred, y)\n",
    "        self.test_confm.update(y_pred, y)\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict(self.test_metrics.compute())\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "        self.test_confm.compute()\n",
    "        fig_, _ = self.test_confm.plot();\n",
    "        plt.show()\n",
    "        self.test_confm.reset()\n",
    "        for logger in self.loggers:\n",
    "            if isinstance(logger, WandbLogger):\n",
    "                wandb.log({\"test_confusion\": fig_})\n",
    "        plt.clf();\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(params = self.model.parameters(),\n",
    "                              lr = self.learning_rate,\n",
    "                              momentum = self.momentum, \n",
    "                              weight_decay = self.weight_decay)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_checkpoint = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINTS_DIR,\n",
    "    filename=\"{epoch}-{train_loss:2f}-{val_loss:2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "local_logger = CSVLogger(\n",
    "    save_dir=Path.cwd(),\n",
    "    name=\"logs\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "#wandb.finish()\n",
    "wandb_logger = WandbLogger(\n",
    "    save_dir=LOGS_DIR,\n",
    "    project=\"ilsvrc\",\n",
    "    #log_model=True,\n",
    "    name=\"alexnet-logging-test\",\n",
    "    version='1',\n",
    "    offline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_class_weights = torch.tensor([0.0982, 0.0990, 0.0952, 0.1102, 0.1005, 0.0989, 0.0984, 0.1016, 0.0994, 0.0985])\n",
    "#imagenette_class_weights = torch.tensor([0.0765, 0.0830, 0.0568, 0.2547, 0.0962, 0.0822, 0.0780, 0.1072, 0.0866, 0.0788])\n",
    "#imagenette_class_weights = torch.tensor([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1])\n",
    "\n",
    "experiment = Hyperparameters(\n",
    "    task = \"multiclass_classification\",\n",
    "    random_seed = 42,\n",
    "    num_classes = 10,\n",
    "    metrics = [\"accuracy\", \"f1\"],\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=imagenette_class_weights),\n",
    "    optimizer = torch.optim.SGD,\n",
    "    learning_rate = 1e-4,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 5e-4,\n",
    "\n",
    "    batch_size = 128,\n",
    "    grad_accum = 4,\n",
    "    test_split = .3,\n",
    "    transform = [\"scale_[0,1]\", \"resize_256\", \"random_crop_224\", \"normalize\", \"hflip\"],\n",
    "    num_workers = 8,\n",
    ")\n",
    "seed_everything(experiment.random_seed, workers=True);\n",
    "alexnet = AlexNet(num_classes=experiment.num_classes, dropout=0)\n",
    "\n",
    "alexnet_transform = t.Compose([\n",
    "    t.ToImage(),\n",
    "    t.ToDtype(torch.float32, scale = True),\n",
    "    t.Resize(size = (256, 256), antialias = True),\n",
    "    t.RandomCrop(size = (224, 224), pad_if_needed = True),\n",
    "    t.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "    t.RandomHorizontalFlip(p = .5)\n",
    "])\n",
    "imagenette_dm = ImagenetteDataModule(\n",
    "        root = IMAGENETTE, \n",
    "        params = experiment, \n",
    "        transform = alexnet_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    #fast_dev_run=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    #limit_train_batches=1,\n",
    "    #limit_val_batches=2,\n",
    "    #deterministic=True,\n",
    "    #benchmark=True\n",
    "    #enable_checkpointing=False,\n",
    "    callbacks=[local_checkpoint],\n",
    "    logger=[local_logger, wandb_logger],\n",
    "\n",
    "    max_epochs = 15,\n",
    "    accumulate_grad_batches = experiment.grad_accum,\n",
    "    check_val_every_n_epoch = 1 \n",
    ")\n",
    "\n",
    "last_ckpt = CHECKPOINTS_DIR / \"last.ckpt\"\n",
    "last_ckpt = last_ckpt if last_ckpt.is_file() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=ClassificationModel(alexnet, experiment),\n",
    "    datamodule=imagenette_dm,\n",
    "    ckpt_path=last_ckpt #type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(\n",
    "    model=ClassificationModel(alexnet, experiment),\n",
    "    datamodule=imagenette_dm,\n",
    "    ckpt_path=last_ckpt,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
